model_checkpoint:
  _target_: lightning.pytorch.callbacks.ModelCheckpoint
  dirpath: null  # Uses default: logs/lightning_logs/version_X/checkpoints/
  filename: "epoch={epoch:03d}-step={step}-val_loss={val_loss:.6f}"
  monitor: "val_loss"
  mode: "min"
  save_top_k: 3  # Keep 3 best checkpoints based on val_loss
  save_last: true  # Always save the last checkpoint
  every_n_epochs: 1  # Save every epoch
  save_on_train_epoch_end: false  # Save on validation end instead
  
early_stopping:
  _target_: lightning.pytorch.callbacks.EarlyStopping
  monitor: "val_loss"
  mode: "min"
  patience: 20  # Stop if val_loss doesn't improve for 20 epochs
  min_delta: 1e-6  # Minimum change to qualify as improvement
  verbose: true