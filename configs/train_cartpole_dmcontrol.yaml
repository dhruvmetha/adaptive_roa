# CartPole DeepMind Control Suite - Latent Conditional Flow Matching (Facebook FM)
# Uses: ℝ × S¹ × ℝ² manifold (same as regular CartPole), GeodesicProbPath, RiemannianODESolver
#
# Key Differences from Regular CartPole:
# - Data has UNWRAPPED theta (up to ±130 rad), wrapped during loading
# - Radius-based success criterion (no failure thresholds)
# - Higher velocity bounds from dm_control physics
# - Can REUSE CartPoleUNet model (same manifold structure!)

defaults:
  - system: cartpole_dmcontrol
  - _self_

name: cartpole_dmcontrol_latent_conditional_fm
seed: 42
batch_size: 256
val_batch_size: 2048  # Validation batch size (can be larger than training)
base_lr: 1e-4
num_workers: 4

# Flow Matcher: Facebook FM version with automatic geodesics
flow_matcher:
  _target_: src.flow_matching.cartpole_dmcontrol.latent_conditional.flow_matcher.CartPoleDMControlLatentConditionalFlowMatcher
  noise_std: 0.01  # Standard deviation for Gaussian noise sampling
  # Note: system, model, optimizer, scheduler, config are passed by training script

# Model: CartPole UNet with FiLM (same ℝ × S¹ × ℝ² manifold!)
model:
  _target_: src.model.cartpole_unet_film.CartPoleUNetFiLM
  embedded_dim: 5              # (x_norm, sin θ, cos θ, ẋ_norm, θ̇_norm)
  condition_dim: 5             # Embedded start state
  time_emb_dim: 128            # Time embedding dimension
  hidden_dims: [256, 512, 512, 256]  # UNet hidden layers
  output_dim: 4                # Velocity in tangent space (dx, dθ, dẋ, dθ̇)
  use_input_embeddings: false
  input_emb_dim: 128
  film_cond_dim: 256           # FiLM conditioning dimension
  film_hidden_dims: []         # FiLM encoder hidden layers (empty = direct projection)
  dropout_p: 0.0               # Dropout probability
  residual_scale: null         # Auto-compute based on num_blocks
  zero_init_blocks: false      # Zero-initialize residual blocks
  zero_init_out: false         # Zero-initialize output projection

# Data: CartPole DM Control endpoint dataset (metadata format)
# NOTE: You need to create metadata files first using roa_labels.txt
data:
  _target_: src.data.cartpole_dmcontrol_endpoint_data.CartPoleDMControlEndpointDataModule
  train_file: /common/users/dm1487/arcmg_datasets/cartpole_dmcontrol/incremental_endpoint_dataset/train_endpoint_dataset.txt
  validation_file: /common/users/dm1487/arcmg_datasets/cartpole_dmcontrol/incremental_endpoint_dataset/val_balanced_endpoint_dataset.txt
  test_file: /common/users/dm1487/arcmg_datasets/cartpole_dmcontrol/incremental_endpoint_dataset/test_balanced_endpoint_dataset.txt
  batch_size: ${batch_size}
  val_batch_size: ${val_batch_size}
  num_workers: ${num_workers}
  shuffle: true
  use_stratified_sampling: false  # Enable stratified sampling for class balance

# Optimizer: AdamW
optimizer:
  _target_: torch.optim.AdamW
  lr: ${base_lr}
  weight_decay: 1e-5
  betas: [0.9, 0.999]

# Scheduler: Reduce on plateau
scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  mode: min
  factor: 0.5
  patience: 10
  min_lr: 1e-6

# Trainer: PyTorch Lightning
trainer:
  _target_: lightning.pytorch.Trainer
  max_epochs: 2000
  accelerator: gpu
  devices: [0]
  precision: 32
  gradient_clip_val: 1.0
  log_every_n_steps: 10
  check_val_every_n_epoch: 1
  enable_progress_bar: true
  enable_model_summary: true
  logger:
    _target_: lightning.pytorch.loggers.TensorBoardLogger
    save_dir: "${hydra:runtime.output_dir}"
    name: ""
    version: null
  callbacks:
    - _target_: lightning.pytorch.callbacks.ModelCheckpoint
      dirpath: "${hydra:runtime.output_dir}/version_0/checkpoints"
      monitor: val_loss
      mode: min
      save_top_k: 3
      save_last: true
      filename: "epoch{epoch:02d}-val_loss{val_loss:.4f}"
      auto_insert_metric_name: false
    - _target_: src.callbacks.best_classification_checkpoint.BestClassificationCheckpoint
      dirpath: "${hydra:runtime.output_dir}/version_0/checkpoints"
      filename: "best_classification-epoch{epoch:02d}-mean{test_classification_mean:.4f}"
      save_top_k: 3
    - _target_: lightning.pytorch.callbacks.EarlyStopping
      monitor: val_loss
      mode: min
      patience: 500
      verbose: true

# Flow matching specific settings
flow_matching:
  num_integration_steps: 100       # ODE solver steps for inference
  mae_val_frequency: 1            # Compute MAE on test set every N epochs

# Hydra output directory
hydra:
  run:
    dir: outputs/${name}/${now:%Y-%m-%d_%H-%M-%S}
