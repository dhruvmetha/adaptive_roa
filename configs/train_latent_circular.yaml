defaults:
  - _self_
  - data: circular_endpoint_data
  - model: latent_circular_mlp
  - module: latent_circular
  - optimizer: adamw
  - scheduler: reduce_lr_on_plateau
  - device: gpu1

seed: 42
batch_size: 2048
num_workers: 4

# Add missing variables for trainer
max_epochs: 200
check_val_every_n_epoch: 1

latent:
  latent_dim: 8
  kl_weight: 1e-4

# Define trainer directly to avoid interpolation issues
trainer:
  _target_: lightning.pytorch.Trainer
  max_epochs: ${max_epochs}
  accelerator: gpu
  devices: 1
  precision: 32
  gradient_clip_val: 1.0
  val_check_interval: 1.0
  check_val_every_n_epoch: ${check_val_every_n_epoch}
  log_every_n_steps: 10
  enable_progress_bar: true
  enable_model_summary: true
  logger:
    _target_: lightning.pytorch.loggers.TensorBoardLogger
    save_dir: "outputs"
    name: "latent_circular_fm"
  callbacks:
    - _target_: lightning.pytorch.callbacks.ModelCheckpoint
      dirpath: "outputs/latent_circular_fm/checkpoints"
      filename: "epoch={epoch:03d}-step={step}-val_loss={val_loss:.6f}"
      monitor: "val_loss"
      mode: "min"
      save_top_k: 3
      save_last: true
      every_n_epochs: 1
    - _target_: lightning.pytorch.callbacks.EarlyStopping
      monitor: "val_loss"
      mode: "min"
      patience: 20
      min_delta: 1e-6
      verbose: true