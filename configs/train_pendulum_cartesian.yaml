# Pendulum Cartesian Latent Conditional Flow Matching (Facebook FM)
# Uses: ℝ⁴ manifold (pure Euclidean), GeodesicProbPath, RiemannianODESolver

defaults:
  - system: pendulum_cartesian
  - _self_

name: pendulum_cartesian_latent_conditional_fm
seed: 42
batch_size: 512
val_batch_size: 2048  # Validation batch size (can be larger than training)
base_lr: 1e-4
num_workers: 4
latent_dim: 2  # Higher latent dim for more complex 4D system

# Flow Matcher: Facebook FM version with automatic geodesics
flow_matcher:
  _target_: src.flow_matching.pendulum_cartesian.latent_conditional.flow_matcher.PendulumCartesianLatentConditionalFlowMatcher
  # Note: system, model, optimizer, scheduler, config, latent_dim are passed by training script

# Model: Latent Conditional UNet for Pendulum Cartesian
model:
  _target_: src.model.pendulum_cartesian_unet.PendulumCartesianUNet
  embedded_dim: 4              # (x, y, vx, vy) - pure Euclidean
  latent_dim: ${latent_dim}    # Gaussian latent variable
  condition_dim: 4             # Embedded start state
  time_emb_dim: 128            # Time embedding dimension
  hidden_dims: [256, 512, 512, 256] # UNet hidden layers
  output_dim: 4                # Velocity in tangent space (dx/dt, dy/dt, dvx/dt, dvy/dt)
  use_input_embeddings: false
  input_emb_dim: 128

# Data: Pendulum Cartesian endpoint dataset
data:
  _target_: src.data.pendulum_cartesian_endpoint_data.PendulumCartesianEndpointDataModule
  data_file: /common/users/dm1487/arcmg_datasets/pendulum_cartesian/incremental_endpoint_dataset/train_100_endpoint_dataset.txt
  validation_file: /common/users/dm1487/arcmg_datasets/pendulum_cartesian/incremental_endpoint_dataset/val_endpoint_dataset.txt
  test_file: /common/users/dm1487/arcmg_datasets/pendulum_cartesian/incremental_endpoint_dataset/test_endpoint_dataset.txt
  batch_size: ${batch_size}
  val_batch_size: ${val_batch_size}
  num_workers: ${num_workers}

# Optimizer: AdamW
optimizer:
  _target_: torch.optim.AdamW
  lr: ${base_lr}
  weight_decay: 1e-5
  betas: [0.9, 0.999]

# Scheduler: Reduce on plateau
scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  mode: min
  factor: 0.5
  patience: 10
  min_lr: 1e-6

# Trainer: PyTorch Lightning
trainer:
  _target_: lightning.pytorch.Trainer
  max_epochs: 2000
  accelerator: gpu
  devices: [0]
  precision: 32
  gradient_clip_val: 1.0
  log_every_n_steps: 10
  check_val_every_n_epoch: 1
  enable_progress_bar: true
  enable_model_summary: true

# Callbacks
callbacks:
  checkpoint:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    dirpath: "${hydra:runtime.output_dir}/version_0/checkpoints"
    monitor: val_loss
    mode: min
    save_top_k: 3
    save_last: true
    filename: "epoch{epoch:02d}-val_loss{val_loss:.4f}"
    auto_insert_metric_name: false
  early_stopping:
    _target_: lightning.pytorch.callbacks.EarlyStopping
    monitor: val_loss
    mode: min
    patience: 500
    verbose: true

# Flow matching specific settings
flow_matching:
  latent_dim: ${latent_dim}           # Dimension of Gaussian latent variable
  num_integration_steps: 100          # ODE solver steps for inference
  mae_val_frequency: 10               # Compute MAE validation every N epochs

# Hydra output directory
hydra:
  run:
    dir: outputs/${name}/${now:%Y-%m-%d_%H-%M-%S}
