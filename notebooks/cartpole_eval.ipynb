{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d568aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CartPole bounds from: /common/users/dm1487/arcmg_datasets/cartpole_pybullet/cartpole_pybullet_data_bounds.pkl\n",
      "Use dynamic bounds: True\n",
      "Path exists: True\n",
      "  [0] Cart position (x): [-5.648, 5.843] -> limit: ¬±5.843\n",
      "  [1] Pole angle (Œ∏): [-3.142, 3.142] -> WRAPPED to ¬±œÄ\n",
      "  [2] Cart velocity (·∫ã): [-6.859, 7.039] -> limit: ¬±7.039\n",
      "  [3] Angular velocity (Œ∏Ãá): [-8.039, 7.946] -> limit: ¬±8.039\n",
      "Loaded CartPole bounds from: /common/users/dm1487/arcmg_datasets/cartpole_pybullet/cartpole_pybullet_data_bounds.pkl\n",
      "üìÅ Folder provided: /common/home/dm1487/robotics_research/tripods/olympics-classifier/outputs/cartpole_old_100_manifold/2025-11-11_03-12-53\n",
      "üîç Searching for checkpoint in folder...\n",
      "   ‚úì Found best checkpoint (val_loss=0.0937)\n",
      "   üìÑ Using: epoch257-val_loss0.0937.ckpt\n",
      "ü§ñ Loading CartPole LCFM checkpoint: /common/home/dm1487/robotics_research/tripods/olympics-classifier/outputs/cartpole_old_100_manifold/2025-11-11_03-12-53/version_0/checkpoints/epoch257-val_loss0.0937.ckpt\n",
      "üìç Device: cuda:0\n",
      "üóÇÔ∏è  Training directory: /common/home/dm1487/robotics_research/tripods/olympics-classifier/outputs/cartpole_old_100_manifold/2025-11-11_03-12-53\n",
      "üìã Loading Hydra config: /common/home/dm1487/robotics_research/tripods/olympics-classifier/outputs/cartpole_old_100_manifold/2025-11-11_03-12-53/.hydra/config.yaml\n",
      "‚úÖ Hydra config loaded successfully\n",
      "üì¶ Loading Lightning checkpoint...\n",
      "‚úÖ Lightning checkpoint loaded\n",
      "üìã Config source: checkpoint (model_config)\n",
      "üìã Final config - latent_dim: 2\n",
      "üìã Model config keys: ['_target_', 'embedded_dim', 'latent_dim', 'condition_dim', 'time_emb_dim', 'hidden_dims', 'output_dim', 'use_input_embeddings', 'input_emb_dim']\n",
      "üîß Creating new CartPole system (not found in hparams)\n",
      "   Using system config from Hydra config\n",
      "   bounds_file: /common/users/dm1487/arcmg_datasets/cartpole/new_cartpole_data_bounds.pkl\n",
      "   use_dynamic_bounds: True\n",
      "Loading CartPole bounds from: /common/users/dm1487/arcmg_datasets/cartpole/new_cartpole_data_bounds.pkl\n",
      "Use dynamic bounds: True\n",
      "Path exists: True\n",
      "  [0] Cart position (x): [-6.015, 6.025] -> limit: ¬±6.025\n",
      "  [1] Pole angle (Œ∏): [-3.142, 3.142] -> WRAPPED to ¬±œÄ\n",
      "  [2] Cart velocity (·∫ã): [-7.048, 7.091] -> limit: ¬±7.091\n",
      "  [3] Angular velocity (Œ∏Ãá): [-8.068, 8.363] -> limit: ¬±8.363\n",
      "Loaded CartPole bounds from: /common/users/dm1487/arcmg_datasets/cartpole/new_cartpole_data_bounds.pkl\n",
      "‚úÖ Initialized CartPole LCFM with Facebook Flow Matching:\n",
      "   - Manifold: ‚Ñù¬≤√óS¬π√ó‚Ñù (Euclidean √ó FlatTorus √ó Euclidean)\n",
      "   - Path: GeodesicProbPath with CondOTScheduler\n",
      "   - Latent dim: 2\n",
      "   - MAE validation frequency: every 10 epochs\n",
      "üîÑ Loading model state dict...\n",
      "\n",
      "‚úÖ Model loaded successfully!\n",
      "   Checkpoint: epoch257-val_loss0.0937.ckpt\n",
      "   Config sources: Hydra + Lightning\n",
      "   System: CartPoleSystem\n",
      "   System bounds: cart¬±6.0, vel¬±7.1\n",
      "   Latent dim: 2\n",
      "   Model architecture: [256, 512, 1024, 512, 256]\n",
      "   Total parameters: 1,333,764\n",
      "   Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from src.systems.cartpole import CartPoleSystem\n",
    "from src.flow_matching.cartpole.latent_conditional.flow_matcher import CartPoleLatentConditionalFlowMatcher\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "data_dir = \"/common/users/shared/pracsys/genMoPlan/data_trajectories/cartpole_pybullet\"\n",
    "roa_file = \"/common/users/shared/pracsys/genMoPlan/data_trajectories/cartpole_pybullet/roa_labels.txt\"\n",
    "bounds_file = \"/common/users/dm1487/arcmg_datasets/cartpole_pybullet/cartpole_pybullet_data_bounds.pkl\"\n",
    "\n",
    "\n",
    "system = CartPoleSystem(bounds_file=bounds_file, use_dynamic_bounds=True)\n",
    "\n",
    "ckpt_path = \"/common/home/dm1487/robotics_research/tripods/olympics-classifier/outputs/cartpole_old_100_manifold/2025-11-11_03-12-53\"\n",
    "flow_matcher = CartPoleLatentConditionalFlowMatcher.load_from_checkpoint(ckpt_path, device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b2c4f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17968548373221382"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roa_data = np.loadtxt(roa_file, delimiter=\",\")\n",
    "inp, labels = roa_data[:, :-1], roa_data[:, -1]\n",
    "inp = torch.from_numpy(inp).float().to(\"cuda:0\")\n",
    "labels = torch.from_numpy(labels).long().to(\"cuda:0\")\n",
    "np.mean(roa_data[:, -1] == 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b6c19f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [09:51<00:00, 10.39s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "samples = 100\n",
    "repeats = 1\n",
    "batch_size = 2048  # You can tune this depending on memory\n",
    "success_threshold = 0.6\n",
    "failure_threshold = 0.4\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "sep_count = 0\n",
    "\n",
    "start_idx = 0\n",
    "stop_idx = len(roa_data)\n",
    "\n",
    "is_success = np.zeros((len(roa_data), samples, repeats))\n",
    "for batch_start in tqdm(range(start_idx, stop_idx, batch_size)):\n",
    "    batch_end = min(batch_start + batch_size, stop_idx)\n",
    "    batch_inp = inp[batch_start:batch_end, :]\n",
    "\n",
    "    # Will be (batch_size, samples, repeats)\n",
    "    for sample_idx in range(samples):\n",
    "        model_input = batch_inp.clone()\n",
    "        for repeat_idx in range(repeats):\n",
    "            pred = flow_matcher.predict_endpoint(model_input)\n",
    "            # pred shape: (batch_size, d)\n",
    "            is_success[batch_start:batch_end, sample_idx, repeat_idx] = system.classify_attractor(pred, 0.2).cpu().numpy()\n",
    "            # is_success[batch_start:batch_end, sample_idx, repeat_idx] = pred[:, 21].cpu().numpy() > 1.3\n",
    "            model_input = pred.clone()\n",
    "            \n",
    "\n",
    "# is_success_mean = is_success.mean(axis=(1,2))\n",
    "# pred_success = (is_success_mean > success_threshold)\n",
    "# pred_failure = (is_success_mean < failure_threshold) \n",
    "\n",
    "# # Compute tp, tn, fp, fn\n",
    "# batch_labels = labels[start_idx:stop_idx].cpu().numpy()\n",
    "# tp = np.sum((batch_labels == 1) & pred_success)\n",
    "# fp = np.sum((batch_labels == 0) & pred_success)\n",
    "# fn = np.sum((batch_labels == 1) & pred_failure)\n",
    "# tn = np.sum((batch_labels == 0) & pred_failure)\n",
    "# sep_count = np.sum((is_success_mean <= success_threshold) & (is_success_mean >= failure_threshold))\n",
    "#     # # Success logic: check along [samples, repeats] for each data point in batch\n",
    "#     # # We'll take the mean across all samples and repeats for head height > 1.3\n",
    "    \n",
    "#     # mean_success = (all_head_heights > 1.3).mean(axis=(1,2))\n",
    "#     # is_success = mean_success > success_threshold  # shape: (batch_size,)\n",
    "    \n",
    "#     # is_failure = mean_success < failure_threshold  # shape: (batch_size,)\n",
    "    \n",
    "#     # sep_count += np.sum((mean_success <= success_threshold) & (mean_success >= failure_threshold))\n",
    "\n",
    "#     # batch_labels = labels[batch_start:batch_end].cpu().numpy()  # shape: (batch_size,)\n",
    "\n",
    "#     # tp += np.sum((batch_labels == 1) & (is_success))\n",
    "#     # fp += np.sum((batch_labels == 0) & (is_success))\n",
    "#     # fn += np.sum((batch_labels == 1) & (is_failure))  \n",
    "#     # tn += np.sum((batch_labels == 0) & (is_failure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3926764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_labels = labels[start_idx:stop_idx].cpu().numpy()\n",
    "pred_labels = np.ones_like(batch_labels) * -1\n",
    "\n",
    "failure = (is_success == -1).sum(axis=(1, 2))/samples > 0.6\n",
    "success = (is_success == 1).sum(axis=(1, 2))/samples > 0.6\n",
    "\n",
    "pred_labels[failure] = 0\n",
    "pred_labels[success] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d4c378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = np.sum((batch_labels == 1) & (pred_labels == 1))\n",
    "tn = np.sum((batch_labels == 0) & (pred_labels == 0))\n",
    "fp = np.sum((batch_labels == 0) & (pred_labels == 1))\n",
    "fn = np.sum((batch_labels == 1) & (pred_labels == 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aded988b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0, Recall: 0, F1: 0, Specificity: 1.0, Sep: 0.0\n"
     ]
    }
   ],
   "source": [
    "precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "specificity = tn / (tn + fp) if tn + fp > 0 else 0\n",
    "sep_perc = sep_count/len(roa_data)\n",
    "print(f\"Precision: {precision}, Recall: {recall}, F1: {f1}, Specificity: {specificity}, Sep: {sep_perc}\")\n",
    "\n",
    "# confusion matrix\n",
    "conf_mat = np.zeros((2, 2))\n",
    "conf_mat[0, 0] = tp\n",
    "conf_mat[0, 1] = fp\n",
    "conf_mat[1, 0] = fn\n",
    "conf_mat[1, 1] = tn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41ac509c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9676020715404071"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pred_labels == -1) / len(roa_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f5063f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d34218",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
