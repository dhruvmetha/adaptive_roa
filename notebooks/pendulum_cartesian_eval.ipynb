{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d568aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pendulum Cartesian bounds from: /common/users/dm1487/arcmg_datasets/pendulum_cartesian/pendulum_cartesian_data_bounds.pkl\n",
      "Use dynamic bounds: True\n",
      "Path exists: True\n",
      "  [0] X Position: [-1.000, 1.000] -> limit: ¬±1.000\n",
      "  [1] Y Position: [-1.000, 1.000] -> limit: ¬±1.000\n",
      "  [2] X Velocity: [-6.282, 6.283] -> limit: ¬±6.283\n",
      "  [3] Y Velocity: [-6.283, 6.283] -> limit: ¬±6.283\n",
      "Loaded Pendulum Cartesian bounds from: /common/users/dm1487/arcmg_datasets/pendulum_cartesian/pendulum_cartesian_data_bounds.pkl\n",
      "üìÅ Folder provided: /common/home/dm1487/robotics_research/tripods/olympics-classifier/outputs/pendulum_cartesian_500/2025-11-11_01-39-41\n",
      "üîç Searching for checkpoint in folder...\n",
      "   ‚úì Found best checkpoint (val_loss=0.0025)\n",
      "   üìÑ Using: epoch1004-val_loss0.0025.ckpt\n",
      "ü§ñ Loading Pendulum Cartesian LCFM checkpoint: /common/home/dm1487/robotics_research/tripods/olympics-classifier/outputs/pendulum_cartesian_500/2025-11-11_01-39-41/version_0/checkpoints/epoch1004-val_loss0.0025.ckpt\n",
      "üìç Device: cuda:0\n",
      "üóÇÔ∏è  Training directory: /common/home/dm1487/robotics_research/tripods/olympics-classifier/outputs/pendulum_cartesian_500/2025-11-11_01-39-41\n",
      "üìã Loading Hydra config: /common/home/dm1487/robotics_research/tripods/olympics-classifier/outputs/pendulum_cartesian_500/2025-11-11_01-39-41/.hydra/config.yaml\n",
      "‚úÖ Hydra config loaded successfully\n",
      "üì¶ Loading Lightning checkpoint...\n",
      "‚úÖ Lightning checkpoint loaded\n",
      "üìã Config source: checkpoint (model_config)\n",
      "üìã Final config - latent_dim: 2\n",
      "üìã Model config keys: ['_target_', 'embedded_dim', 'latent_dim', 'condition_dim', 'time_emb_dim', 'hidden_dims', 'output_dim', 'use_input_embeddings', 'input_emb_dim']\n",
      "üîß Creating new Pendulum Cartesian system (not found in hparams)\n",
      "   Using system config from Hydra config\n",
      "   bounds_file: /common/users/dm1487/arcmg_datasets/pendulum_cartesian/pendulum_cartesian_data_bounds.pkl\n",
      "   use_dynamic_bounds: True\n",
      "Loading Pendulum Cartesian bounds from: /common/users/dm1487/arcmg_datasets/pendulum_cartesian/pendulum_cartesian_data_bounds.pkl\n",
      "Use dynamic bounds: True\n",
      "Path exists: True\n",
      "  [0] X Position: [-1.000, 1.000] -> limit: ¬±1.000\n",
      "  [1] Y Position: [-1.000, 1.000] -> limit: ¬±1.000\n",
      "  [2] X Velocity: [-6.282, 6.283] -> limit: ¬±6.283\n",
      "  [3] Y Velocity: [-6.283, 6.283] -> limit: ¬±6.283\n",
      "Loaded Pendulum Cartesian bounds from: /common/users/dm1487/arcmg_datasets/pendulum_cartesian/pendulum_cartesian_data_bounds.pkl\n",
      "‚úÖ Initialized Pendulum Cartesian LCFM with Facebook Flow Matching:\n",
      "   - Manifold: ‚Ñù‚Å¥ (Pure Euclidean)\n",
      "   - Path: GeodesicProbPath with CondOTScheduler\n",
      "   - Latent dim: 2\n",
      "   - MAE validation frequency: every 10 epochs\n",
      "üîÑ Loading model state dict...\n",
      "\n",
      "‚úÖ Model loaded successfully!\n",
      "   Checkpoint: epoch1004-val_loss0.0025.ckpt\n",
      "   Config sources: Hydra + Lightning\n",
      "   System: PendulumCartesianSystem\n",
      "   System bounds: x¬±1.0, y¬±1.0, vx¬±6.3, vy¬±6.3\n",
      "   Latent dim: 2\n",
      "   Model architecture: [256, 512, 512, 256]\n",
      "   Total parameters: 562,180\n",
      "   Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from src.systems.pendulum_cartesian import PendulumCartesianSystem\n",
    "from src.flow_matching.pendulum_cartesian.latent_conditional.flow_matcher import PendulumCartesianLatentConditionalFlowMatcher\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "data_dir = \"/common/users/shared/pracsys/genMoPlan/data_trajectories/pendulum_cartesian_50k\"\n",
    "roa_file = \"/common/users/shared/pracsys/genMoPlan/data_trajectories/pendulum_cartesian_50k/roa_labels.txt\"\n",
    "bounds_file = \"/common/users/dm1487/arcmg_datasets/pendulum_cartesian/pendulum_cartesian_data_bounds.pkl\"\n",
    "\n",
    "\n",
    "system = PendulumCartesianSystem(bounds_file=bounds_file, use_dynamic_bounds=True)\n",
    "\n",
    "ckpt_path = \"/common/home/dm1487/robotics_research/tripods/olympics-classifier/outputs/pendulum_cartesian_500/2025-11-11_01-39-41\"\n",
    "flow_matcher = PendulumCartesianLatentConditionalFlowMatcher.load_from_checkpoint(ckpt_path, device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b2c4f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38585493269037574"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roa_data = np.loadtxt(roa_file, delimiter=\",\")\n",
    "inp, labels = roa_data[:, :-1], roa_data[:, -1]\n",
    "inp = torch.from_numpy(inp).float().to(\"cuda:0\")\n",
    "labels = torch.from_numpy(labels).long().to(\"cuda:0\")\n",
    "np.mean(roa_data[:, -1] == 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b6c19f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [01:31<00:00,  3.65s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "samples = 100\n",
    "repeats = 1\n",
    "batch_size = 2048  # You can tune this depending on memory\n",
    "success_threshold = 0.6\n",
    "failure_threshold = 0.4\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "sep_count = 0\n",
    "\n",
    "start_idx = 0\n",
    "stop_idx = len(roa_data)\n",
    "\n",
    "is_success = np.zeros((len(roa_data), samples, repeats))\n",
    "for batch_start in tqdm(range(start_idx, stop_idx, batch_size)):\n",
    "    batch_end = min(batch_start + batch_size, stop_idx)\n",
    "    batch_inp = inp[batch_start:batch_end, :]\n",
    "\n",
    "    # Will be (batch_size, samples, repeats)\n",
    "    for sample_idx in range(samples):\n",
    "        model_input = batch_inp.clone()\n",
    "        for repeat_idx in range(repeats):\n",
    "            pred = flow_matcher.predict_endpoint(model_input)\n",
    "            # pred shape: (batch_size, d)\n",
    "            is_success[batch_start:batch_end, sample_idx, repeat_idx] = system.classify_attractor(pred, 0.1).cpu().numpy()\n",
    "            # is_success[batch_start:batch_end, sample_idx, repeat_idx] = pred[:, 21].cpu().numpy() > 1.3\n",
    "            model_input = pred.clone()\n",
    "            \n",
    "\n",
    "# is_success_mean = is_success.mean(axis=(1,2))\n",
    "# pred_success = (is_success_mean > success_threshold)\n",
    "# pred_failure = (is_success_mean < failure_threshold) \n",
    "\n",
    "# # Compute tp, tn, fp, fn\n",
    "# batch_labels = labels[start_idx:stop_idx].cpu().numpy()\n",
    "# tp = np.sum((batch_labels == 1) & pred_success)\n",
    "# fp = np.sum((batch_labels == 0) & pred_success)\n",
    "# fn = np.sum((batch_labels == 1) & pred_failure)\n",
    "# tn = np.sum((batch_labels == 0) & pred_failure)\n",
    "# sep_count = np.sum((is_success_mean <= success_threshold) & (is_success_mean >= failure_threshold))\n",
    "#     # # Success logic: check along [samples, repeats] for each data point in batch\n",
    "#     # # We'll take the mean across all samples and repeats for head height > 1.3\n",
    "    \n",
    "#     # mean_success = (all_head_heights > 1.3).mean(axis=(1,2))\n",
    "#     # is_success = mean_success > success_threshold  # shape: (batch_size,)\n",
    "    \n",
    "#     # is_failure = mean_success < failure_threshold  # shape: (batch_size,)\n",
    "    \n",
    "#     # sep_count += np.sum((mean_success <= success_threshold) & (mean_success >= failure_threshold))\n",
    "\n",
    "#     # batch_labels = labels[batch_start:batch_end].cpu().numpy()  # shape: (batch_size,)\n",
    "\n",
    "#     # tp += np.sum((batch_labels == 1) & (is_success))\n",
    "#     # fp += np.sum((batch_labels == 0) & (is_success))\n",
    "#     # fn += np.sum((batch_labels == 1) & (is_failure))  \n",
    "#     # tn += np.sum((batch_labels == 0) & (is_failure))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3926764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_labels = labels[start_idx:stop_idx].cpu().numpy()\n",
    "pred_labels = np.ones_like(batch_labels) * -1\n",
    "\n",
    "failure = (is_success == -1).sum(axis=(1, 2))/samples > 0.6\n",
    "success = (is_success == 1).sum(axis=(1, 2))/samples > 0.6\n",
    "\n",
    "pred_labels[failure] = 0\n",
    "pred_labels[success] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d4c378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = np.sum((batch_labels == 1) & (pred_labels == 1))\n",
    "tn = np.sum((batch_labels == 0) & (pred_labels == 0))\n",
    "fp = np.sum((batch_labels == 0) & (pred_labels == 1))\n",
    "fn = np.sum((batch_labels == 1) & (pred_labels == 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aded988b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9473771123465624, Recall: 0.9072268198829793, F1: 0.926867359577791, Specificity: 0.9683956494429568, Sep: 0.0\n"
     ]
    }
   ],
   "source": [
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "specificity = tn / (tn + fp)\n",
    "sep_perc = sep_count/len(roa_data)\n",
    "print(f\"Precision: {precision}, Recall: {recall}, F1: {f1}, Specificity: {specificity}, Sep: {sep_perc}\")\n",
    "\n",
    "# confusion matrix\n",
    "conf_mat = np.zeros((2, 2))\n",
    "conf_mat[0, 0] = tp\n",
    "conf_mat[0, 1] = fp\n",
    "conf_mat[1, 0] = fn\n",
    "conf_mat[1, 1] = tn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41ac509c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011050833835643962"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pred_labels == -1) / len(roa_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
